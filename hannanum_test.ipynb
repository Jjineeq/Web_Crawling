{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\version_test\\lib\\site-packages\\ipykernel_launcher.py:14: MatplotlibDeprecationWarning: \n",
      "The get_fontconfig_fonts function was deprecated in Matplotlib 3.5 and will be removed two minor releases later.\n",
      "  \n",
      "c:\\Users\\User\\anaconda3\\envs\\version_test\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "plt.rcParams['font.family'] ='Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] =False\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "fm.get_fontconfig_fonts()\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "import pickle\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.callbacks import CoherenceMetric\n",
    "from gensim.models.callbacks import PerplexityMetric\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "font_loaction = 'C:/Users/User/Desktop/NANUMGOTHIC.ttf'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['음식료품', '섬유의복', '종이목재', '화학', '의약품', '비금속광물', '철강금속', '기계',\n",
    "        '전기전자', '의료정밀', '운수장비', '유통업', '전기가스업', '건설업', '운수창고', '통신업', '금융업',\n",
    "        '은행', '증권', '보험', '서비스업']\n",
    "\n",
    "# category = ['음식료품' ,'섬유의복', '종이목재']\n",
    "ddf = pd.read_csv(\"C:/Users/user/github/Web_Crawling/save_data/cyjhword.csv\")\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ddf['불용어'].values.tolist()\n",
    "negative_words = ddf['부정어'].dropna().values.tolist()\n",
    "positive_words = ddf['긍정어'].dropna().values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 1차 불용어 처리\n",
    "result = pd.DataFrame()\n",
    "for j in tqdm(range(len(category))):\n",
    "    name = category[j]\n",
    "    df = pd.read_csv('C:/Users/user/github/Web_Crawling/save_data/'+'{}'.format(name)+'_total.csv') # 데이터 불러오기\n",
    "    count_positive = 0\n",
    "    count_negative = 0\n",
    "    year_list = []\n",
    "    month_list = []\n",
    "    df['{}_positive'.format(category[j])] = 0\n",
    "    df['{}_negative'.format(category[j])] = 0\n",
    "    line = []\n",
    "    df['time'] = df['time'].astype(str)\n",
    "    df['time']=pd.to_datetime(df.iloc[:,0])\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        count_positive = 0\n",
    "        count_negative = 0\n",
    "        year_list.append(df.iloc[i,0].year)\n",
    "        month_list.append(df.iloc[i,0].month)\n",
    "        a = df.iloc[:,1][i] # 모든 title은 1행에 존재\n",
    "        #h = hannanum.nouns(a) # 형태 분석\n",
    "        #k = kkma.nouns(a)\n",
    "        k1 = kkma.pos(a)\n",
    "        for (k ,tclass) in k1:\n",
    "            if (tclass == 'NNM' or tclass == 'NNG' or tclass=='NNP' or tclass=='NNB' or tclass=='NP') & (len(k)>1):\n",
    "                line.append(k)\n",
    "        z = list(set(line) - set(stop_words))\n",
    "        for b in z:\n",
    "            if b in positive_words:\n",
    "                count_positive += 1\n",
    "            if b in negative_words:\n",
    "                count_negative += 1\n",
    "        df.iloc[i, 3] = count_positive\n",
    "        df.iloc[i, 4] = count_negative\n",
    "    df['year'] = year_list\n",
    "    df['month'] = month_list\n",
    "    if j==0:\n",
    "        first = df.groupby(['year','month'])['{}_negative'.format(category[0]),'{}_positive'.format(category[0])].sum()\n",
    "        first['{}'.format(category[j])+'_negative_rate'] = first.iloc[:,0] / (first.iloc[:,0] + first.iloc[:,1])\n",
    "        first['{}'.format(category[j])+'_positive_rate'] = first.iloc[:,1] / (first.iloc[:,0] + first.iloc[:,1])\n",
    "        result = pd.concat([result, first], axis=1)\n",
    "        add  = pd.DataFrame()\n",
    "    else:\n",
    "        add = df.groupby(['year','month'])['{}_negative'.format(category[j]),'{}_positive'.format(category[j])].sum()\n",
    "        add['{}'.format(category[j])+'_negative_rate'] = add.iloc[:,0] / (add.iloc[:,0] + add.iloc[:,1])\n",
    "        add['{}'.format(category[j])+'_positive_rate'] = add.iloc[:,1] / (add.iloc[:,0] + add.iloc[:,1])\n",
    "    result = pd.concat([result,add],axis=1)  \n",
    "    \n",
    "#     save = pd.DataFrame(z) # 저장할 df만들기\n",
    "#     save.to_csv('C:/Users/user/github/Web_Crawling/csv/'+'{}'.format(name)+'_token.csv', encoding='utf-8-sig',index=False)\n",
    "#     dictionary = corpora.Dictionary(z)                    # 사전 생성 (토큰화)\n",
    "\n",
    "#     corpus = [dictionary.doc2bow(text) for text in z]     # 말뭉치 생성 (벡터화)\n",
    "#     num_topics = 5\n",
    "#     chunksize = 2000\n",
    "#     passes = 20\n",
    "#     iterations = 400\n",
    "#     eval_every = None\n",
    "\n",
    "#     temp = dictionary[0]\n",
    "#     id2word = dictionary.id2token\n",
    "\n",
    "#     model = LdaModel(\n",
    "#         corpus=corpus,\n",
    "#         id2word=id2word,\n",
    "#         chunksize=chunksize,\n",
    "#         alpha='auto',\n",
    "#         eta='auto',\n",
    "#         iterations=iterations,\n",
    "#         num_topics=num_topics,\n",
    "#         passes=passes,\n",
    "#         eval_every=eval_every\n",
    "#         )\n",
    "#     lda_visualization = gensimvis.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "#     pyLDAvis.save_html(lda_visualization,'{}'.format(name) + 'hannanum.html')\n",
    "result.to_csv('C:/Users/user/github/Web_Crawling/save_data/total_count1.csv', encoding='cp949',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "version_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfe135afd72bafd3529fdb2d5bee25c98d4b633c63039531eadf892df31ba1d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
