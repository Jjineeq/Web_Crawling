{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_function(category, start, end, page, header):\n",
    "    \"\"\"\n",
    "    input은 4개입니다.\n",
    "    카테고리는 '검색 이름'\n",
    "    start는 시작날짜 : '20221101' -> 22년 11월 01일부터 검색\n",
    "    end는 종료날짜 : '20221203' -> 22년 12월 03일까지 검색\n",
    "    page는 몇개의 페이지를 저장할건지 : 3 -> 3페이지\n",
    "    header는 크롬에서 F12 누르고 Network -> All -> Headers -> User-Agent 복사해서 dict형태로 넣어주면 됩니다.\n",
    "\n",
    "    ex) \n",
    "    header = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "    news_function('음식료품','20221101','20221203',3, header) \n",
    "    -> 음식료품을 검색해서 22년 11월 1일부터 22년 12월 3일까지 3페이지씩 저장한다.\n",
    "\n",
    "    개인에 맞게 변경해야되는 것은 to_csv 주소입니다. 이것은 개인 컴퓨터에 맞게 변경해야됩니다.\n",
    "    \"\"\"\n",
    "\n",
    "    query = category\n",
    "    header = header \n",
    "    \n",
    "    date_range = pd.date_range(start, end)\n",
    "    \n",
    "    news_title = []\n",
    "    for date in tqdm(date_range):\n",
    "        date_str = date.strftime('%Y%m%d')\n",
    "        for page_num in range(page):\n",
    "            start_idx = page_num * 10 + 1\n",
    "            url = f'https://search.naver.com/search.naver?where=news&query={query}&sort=0&nso=so%3Ar%2Cp%3Afrom{date_str}to{date_str}&start={start_idx}'\n",
    "            res = requests.get(url, headers=header)\n",
    "            soup = bs(res.text, 'lxml')\n",
    "            for title in soup.find_all('a', class_='news_tit'):\n",
    "                news_title.append([date_str, title['title']])\n",
    "                \n",
    "    news_title = pd.DataFrame(news_title).drop_duplicates()\n",
    "    news_title['category'] = query\n",
    "    news_title.columns = ['time', 'title', 'category']\n",
    "    #news_title.to_csv('../Web_Crawling/save_data/' + query + '_total.csv', encoding='utf-8-sig', index=False) # csv 내보내기\n",
    "    return news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20220102</td>\n",
       "      <td>[르포] 이준석 닷새 머문 상주안전체험교육센터...300m 빙판길 올랐더니</td>\n",
       "      <td>빗길사고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20220103</td>\n",
       "      <td>사망사고 내고…운전 안했다던 고교생 결국</td>\n",
       "      <td>빗길사고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20220104</td>\n",
       "      <td>[손바닥문학상 가작 수상작] 화이불변</td>\n",
       "      <td>빗길사고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20220107</td>\n",
       "      <td>“중년의 허전함 달래준 건… 10년만에 다시 잡은 클럽”</td>\n",
       "      <td>빗길사고</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time                                      title category\n",
       "0  20220102  [르포] 이준석 닷새 머문 상주안전체험교육센터...300m 빙판길 올랐더니     빗길사고\n",
       "1  20220103                     사망사고 내고…운전 안했다던 고교생 결국     빗길사고\n",
       "2  20220104                       [손바닥문학상 가작 수상작] 화이불변     빗길사고\n",
       "3  20220107            “중년의 허전함 달래준 건… 10년만에 다시 잡은 클럽”     빗길사고"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_function('빗길사고','20220101', '20220110', 1, {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(query, start, end, page):\n",
    "    link_save = []\n",
    "    \n",
    "    date_range = pd.date_range(start, end)\n",
    "    \n",
    "    for date in tqdm(date_range):\n",
    "        date_str = date.strftime('%Y%m%d')\n",
    "        for page_num in range(page):\n",
    "            start_idx = page_num * 10 + 1\n",
    "            search_url = f'https://search.naver.com/search.naver?where=news&query={query}&sort=0&nso=so%3Ar%2Cp%3Afrom{date_str}to{date_str}&start={start_idx}'\n",
    "        \n",
    "        \n",
    "            response = requests.get(search_url)\n",
    "            soup = bs(response.text, \"html.parser\")\n",
    "            articles = soup.find_all(\"div\", class_=\"news_area\")\n",
    "\n",
    "            for article in articles:\n",
    "                title = article.find(\"a\", class_=\"news_tit\").text.strip()\n",
    "                link = article.find(\"a\", class_=\"news_tit\")[\"href\"]\n",
    "                link_save.append(link)\n",
    "                article_response = requests.get(link)\n",
    "                article_soup = bs(article_response.text, \"html.parser\")\n",
    "\n",
    "                content = article_soup.find(\"div\", class_=\"news_end\")\n",
    "                if content is not None:\n",
    "                    content = content.text.strip()\n",
    "                else:\n",
    "                    content = \"본문을 가져올 수 없습니다.\"\n",
    "\n",
    "                date_articles = article_soup.find(\"span\", class_=\"t11\")\n",
    "                date_articles_element = article_soup.find(\"span\", class_=\"t11\")\n",
    "                if date_articles_element is not None:\n",
    "                    date_articles = date_articles_element.text.strip()\n",
    "                else:\n",
    "                    date_articles = \"날짜를 가져올 수 없습니다.\"\n",
    "\n",
    "                # print(\"제목:\", title)\n",
    "                # print(\"본문:\", content)\n",
    "                # print(\"날짜:\", date_articles)\n",
    "                # print()\n",
    "\n",
    "    return link_save, title, content, date_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = '빗길사고'\n",
    "header = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}\n",
    "start = '20210101'\n",
    "end = '20210105'\n",
    "page = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['https://www.iloveorganic.co.kr/news/articleView.html?idxno=318339',\n",
       "  'http://star.mk.co.kr/new/view.php?mc=ST&year=2021&no=4800',\n",
       "  'http://www.queen.co.kr/news/articleView.html?idxno=349536',\n",
       "  'http://www.worklaw.co.kr/view/view.asp?accessSite=Naver&accessMethod=Search&accessMenu=News&in_cate=119&in_cate2=0&gopage=1&bi_pidx=31758'],\n",
       " '배달음식이 조금 늦게 오더라도',\n",
       " '본문을 가져올 수 없습니다.',\n",
       " '날짜를 가져올 수 없습니다.')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_articles(category, start, end, page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
