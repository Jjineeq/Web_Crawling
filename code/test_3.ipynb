{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0                                1\n",
      "0 2020-01-01   [서울중앙지법, 송병기 경제부시장 구속영장 청구 기각]\n",
      "1 2020-01-01  [[서울광장] 장강의 뒷물결과 검찰개혁/박홍환 논설위원]\n",
      "2 2020-01-01   [김현미 “주거정책 시장 경제 룰에 맡겨서는 안 돼”]\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import lxml\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "\n",
    "category = ['음식료품', '섬유의복', '종이목재', '화학', '의약품', '비금속광물', '철강금속', '기계',\n",
    "       '전기전자', '의료정밀', '운수장비', '유통업', '전기가스업', '건설업', '운수창고', '통신업', '금융업',\n",
    "       '은행', '증권', '보험', '서비스업']\n",
    "\n",
    "result = []\n",
    "query = '건설업'\n",
    "date = '20200101'\n",
    "date = pd.date_range(date,date)\n",
    "\n",
    "page = 3\n",
    "\n",
    "header = {'User-Agent':\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"} \n",
    "cookie = {'CONSENT' : 'YES'}\n",
    "news_all = []\n",
    "news_title=[]\n",
    "for i in date:\n",
    "    date = str(i).replace('-','')[0:8]\n",
    "    #url = 'https://search.naver.com/search.naver?where=news&query=건설업&sort=3&ds=2020.01.01&de=2020.01.31&nso=so%3Ar%2Cp%3Afrom20200101to20200131&start=1'\n",
    "    for j in range(1,page+1):\n",
    "        page = (j-1)*10+1\n",
    "        url = 'https://search.naver.com/search.naver?where=news&query={}&sort=3&nso=so%3Ar%2Cp%3Afrom{}to{}&start={}'.format(query, date, date, page)\n",
    "        res = requests.get(url, headers = header)\n",
    "        soup = bs(res.text, 'lxml')\n",
    "        \n",
    "        for title in soup.find_all('a', attrs={'class':'news_tit'}):\n",
    "            news_title = title\n",
    "            #print(str(news_title))\n",
    "            #print(type(news_title))\n",
    "        news_all.append([i,news_title])\n",
    "\n",
    "#print(news_all)\n",
    "print(pd.DataFrame(news_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(url_q, headers=headers) # 값 받아오기\n",
    "# html_text = response.text # text만 남기기\n",
    "# soup = bs(html_text,'html.parser') # 문서 파싱\n",
    "# link = list(soup.select('li>dl>dt>a')) # li dl dt a 순서로 내려옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.new_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = soup.find_all('a', attrs={'class':'news_tit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m z\u001b[39m.\u001b[39;49mfind()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\crawling\\lib\\site-packages\\bs4\\element.py:2289\u001b[0m, in \u001b[0;36mResultSet.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m   2288\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2289\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m   2290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mResultSet object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. You\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m key\n\u001b[0;32m   2291\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c83ef3adab64e3cbb510cefd6b611928ffaaeddedd527d1b02ef4edcd5a80c0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
